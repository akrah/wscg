% WSCG sample document 
%
% based on Gabriel Zachmann's sample
% http://zach.in.tu-clausthal.de/latex/
%
% modified Apr 2012 to match WSCG Word template
%
\documentclass[twoside,twocolumn,10pt]{article}
%\documentclass[twoside,twocolumn,draft]{article}

%  for debugging
%\tracingall%\tracingonline=0
%\tracingparagraphs
%\tracingpages


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                             Packages

\usepackage{wscg}           % includes a number of other packages (e.g., myalgorithm)
\RequirePackage{ifpdf}
\ifpdf
 \RequirePackage[pdftex]{graphicx}
 \RequirePackage[pdftex]{color}
\else
 \RequirePackage[dvips,draft]{graphicx}
 \RequirePackage[dvips]{color}
\fi
%\usepackage[german,english]{babel}     % default = english
%\usepackage{mypicture}      % loads graphicx.sty, color.sty, eepic.sty
%\usepackage{array}          % better tabular's & arrays, plus math tabular's
%\usepackage{tabularx}      % for selfadjusting p-columns
%\setlength{\extrarowheight}{1ex}   % additional space between rows
%\usepackage{booktabs}      % typographically much better
%\usepackage{mdwlist}        % for compacted lists, and more versatile lists
%\usepackage[intlimits]{amsmath} % more math stuff, see texdoc amsldoc
%\usepackage{mymath}         % own commands, loads amssymb & array.sty
%\usepackage{hyphenat}      % hyphenatable -, /, etc.
%\usepackage{theorem}
%\usepackage[sort&compress]{natbib}% better \cite commands, more flexible
%\usepackage[sort&compress,super]{natbib} % better \cite commands, more flexible
%\newcommand{\citenumfont}[1]{\textit{#1}}


\usepackage{nopageno}       % no page numbers at all; uncomment for final version

\usepackage{subfig}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{amsmath}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                Title

\title{Automatic morphology: Application on biological images}

\author{
  \hspace{-0.1\textwidth}
  \parbox{0.2\textwidth}{\centering
    LE Van \\
    Linh\\[1mm]
LaBRI-CNRS 5800\\
ITDLU, Dalat Univ-V
linhlv@dlu.edu.vn/ van-linh.le@labri.fr
}
\hspace{0.02\textwidth}
\parbox{0.2\textwidth}{\centering
BEURTON-AIMAR Marie\\[1mm]
LaBRI-CNRS 5800\\
Bordeaux University\\
33400 Talence-F\\
beurton@labri.fr
}
\hspace{0.02\textwidth}
\parbox{0.25\textwidth}{\centering
KRAHENBUHL \\Adrien\\[1mm]
LaBRI-CNRS 5800\\
Bordeaux University\\
33400 Talence-F\\
adrien.krahenbuhl@labri.fr
}
\hspace{0.02\textwidth}
\parbox{0.18\textwidth}{\centering
PARISEY\\ Nicolas\\[1mm]
IGEPP\\
INRA 1349\\
35653 Le Rheu-F\\
nparisey@rennes.inra.fr
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                          Hyperref


% no hyperlinks
\usepackage{url}
\urlstyle{tt}

% Donald Arsenau's fix for missing kerning of "//" and ":/"
\makeatletter
\def\Uslash{\mathbin{\mathchar`\/}\@ifnextchar{/}{\kern-.15em}{}}
\g@addto@macro\UrlSpecials{\do \/ {\Uslash}}
\def\Ucolon{\mathbin{\mathchar`:}\@ifnextchar{/}{\kern-.1em}{}}
\g@addto@macro\UrlSpecials{\do : {\Ucolon}}
\makeatother





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              My Commands


%\DeclareMathOperator{\sgn}{sgn}

%\theorembodyfont{\upshape}
%\theoremstyle{break}
%\theoremheaderfont{\bfseries\normalsize}

%\newtheorem{lem}{Lemma}
%\newtheorem{defn}{Definition}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                Document


\begin{document}

\twocolumn[{\csname @twocolumnfalse\endcsname

\maketitle  % full width title


\begin{abstract}
\noindent
  Phenotype of species are characterized by several informations like,
  age, sex, morphological measures and environment parameters. Biologists are
  familar to manually getting morphological measures. In the case of
  analysis at the macro level (tissues, small parts of animal \ldots)
  that can be done directly by measuring the element geometry: length,
  width, diameter, angles \ldots. Another way is to take pictures of
  these elements and to run image processing algorithms. From a
  collection of beetle pictures, we have initiated this work to test the
  feasability of automatically positionning landmarks on biological
  images. The set of images contains 293 beetles and for all an image of the left
  and right mandibles. For each mandible, a set of 16 and 18
  landmarks (resp. left and right) have been set manually. In a previous
  work\cite{leestimating} based on Palanaswami article \cite{palaniswamy2010automatic} we have shown that
  if we consider the centroid measure of the mandible as the parameter
  to obtain, the probabilistic Hough Transform can provide very
  interesting results. But in the case of the main goal is to fix
  landmarks to get property about specific area (defined by
  biologists) the result is not as precised as we have expected. In this
  next turn, we have kept the same method to segment and to register
  scene images with model but we have change the way to set the
  model landmarks on the scene. This new procedure uses a SIFT
  algorithm as the last operation of our process. We will compare all
  the obtained results in order to define the new workflow to label
  the mandible images.
\end{abstract}

\subsection*{Keywords}
Automatic morphology, landmarks identification, image registration.

\vspace*{1.0\baselineskip}
}]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}

\copyrightspace

In biology, morphology analysis is widely used to keep the changing
information of the organism or detecting the difference information
between the organisms. From the result of morphology analysis, we can
conclude the evolution of an organism family, or we may classify the
organisms. Especially in agriculture, morphology is one of best ways
to learn about the variations of the insect on crops. The morphology
methods may be divided into the groups by the features which are used
by the methods such as shape, structure, color, pattern or size of the
object. In the aim to study the potential links between these
variations and agricultual ecosystems, a set of 291 beetles has been
collected with all the information about the sex, place where they are
found and agricultural practices in each field were recorded. For each
beetle, the morphometric landmarks has been defined on each part (each
insect includes five parts: head, pronotum, body, left and right
mandible) of the insect by the biologies. In this context, we try to
indicate the landmarks on two parts of beetle (left and right mandible
(see figure \ref{figparts})). Morphometric landmarks are points that
can be defined in all speciemns and located precisely. Landmarks are
widely used in many biological studies and they are currently included
into the classification procedures.\\[0.1cm]

\begin{figure}[h]
\centering
\subfloat[Left mandible]{\label{figrbox2}\includegraphics[width=0.22\textwidth]{./images/lm}}~~
\subfloat[Right mandible]{\label{figrbox1}\includegraphics[width=0.22\textwidth]{./images/rm}}
\caption{The mandibles of beetle}
\label{figparts}
\end{figure}~\\[0.1cm]
In this paper, we focus on a method that can automatic identification
of landmarks on 2D images of beetle, specify the mandibles of
beetle. Through whole the article, we use two images to automatic
extraction the landmarks: model image and scene image. The method
mainly includes three stages: firstly, we extract the features of the
object in the image; secondly, principal component analysis iteration is 
used to register two images, the translation and
rotation between two images are also determined; finally, a refinement of
the estimated landmarks is done by SIFT method.\\[0.1cm]

In section 2, the steps of our methods will be discussed. All
experiments and evaluation are described in section 3. Finally, we
have some conclusion in section 4.

\section{Method}
Given a model image with its manual landmarks and a scene image, we estimate the location of the scene's landmarks based on the model and its manual landmarks. The landmarks of the model are the points in the image that have been set by biologists corresponding to the morphological points of interest (18 landmarks for each right mandible and 16 landmarks for each left mandible). An overview of the proposed method is shown in figure \ref{fig:method}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/method}
    \caption{Overview of the proposed method}
    \label{fig:method}
\end{figure}
In this section, we will describe three steps in our method: segmentation, registration and refinement the estimated landmarks. During the method, we assume that the input images are regarded as the same size and same part; it means that do not have the confusion between the parts of the beetle and not yet to mention about the scale between the images. 

\subsection{Image segmentation}
In the methods of image processing, feature extraction is always an
important stage. It has a strong decide to the result of the method. During feature extraction, depending on the nature of the method, some techniques are applied before (pre-processing) or after (post-process) the main process extracting the features. In our approach, the original Canny
alogrithm\cite{canny1986computational} is idea for detect the curves on the image. The ratio between lower threshold and upper threshold which are used in Canny algorithm is 1:3 (lower threshold approximated to 1 times
\textit{threshold value} and upper threshold set to 3 times threshold
value). This ratio is evaluated by the experiments. The
\textit{threshold value} has been determined by analyzing the image
histogram (see \cite{leestimating} for detail). During applying the Canny algorithm to detect the curves of
object, the gradient direction of each pixel which belongs to the
curves is kept for the next steps of the method.

\begin{figure}[h]
\centering
\subfloat[Segmentation result after applying Canny
  algorithm]{\label{canny1}\includegraphics[width=0.2\textwidth]{./images/canny1}}~~ 
\subfloat[Segmentation result after applying Canny algorithm and
  post-process]{\label{canny2}\includegraphics[width=0.2\textwidth]{./images/canny2}} 
\caption{The segmentation results of the image}
\label{canny}
\end{figure}~\\
In this study, the aim of segmentation stage is determined the outer
border of the object which can be used to reconstruct the shape of the
object as well as provide the best data for next step. The curves from
Canny algorithm will be post-processed to remove the unnecessary
curves i.e hole inside the border (see figure \ref{canny}).

\subsection{Image registration}
Our study is working on the 2D image, and the transformation between
two images are inevitable. At this step, we will try to align the segmentations of two images before estimating the landmarks. Besides, the translation and rotation between the images are determined by Principal
Component Analysis Iteration (PCAI). PCAI is the method that is improved from PCA method\cite{shlens2014tutorial}. The work flows of PCAI are illustrated in figure \ref{fig:pcai}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{./images/pcadiagram}
    \caption{The flows in PCAI}
    \label{fig:pcai}
\end{figure}~\\
PCAI received the result from segmentation step as the input. Firstly, the centroid point and principal axis of each image are defined: the centroid point is the point which has the coordinate by getting the mean coordinate of all boundary points; the principal axis is a connected line from the centroid point to a point in the list of curve points. The second endpoint on curve points are determined as described: For each point in the curve, we assume that it is the second endpoint of the principal axis. Then, the mean of the perpendicular distance from the remaining points to the axis is calculated. This work is finished for all points on the curve, and the principal axis is the line that has the minimum mean perpendicular distance to other points. The translation is
indicated by the difference coordinate between the centroid points. The rotation angle is the angle between the principal axes. As well as, the
rotation direction is determined by checking on each direction. Then, the scene is moved to matching with the position of the model. However,
in some case, the translation and rotation between two images are
wrong indicated because the result of the segmentation may be not perfect. To make sure that we have obtained the best match
between the images. We extract a sub-set of segmentation points to re-calculate the PCA. Before extracting, the points have been sorted by y-coordinate to suppress the unnecessary points. The iteration is continual until we get the best register of the segmentation.\\

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.2\textwidth]{./images/imreg}
    \caption{A registration result between two images.}
    \label{fig:box}
\end{figure}~\\

Figure \ref{fig:box} shows an example of PCAI. In this figure, the aquamarine curve and blue curve are presented for the segmentation of the model and the scene image, respectively.\\

At the end of PCAI, a hypothesis is made to judge the scale between the images. For each image, the bounding boxes are indicated by the coordinates of the points on the curve. The scale of x and y-direction are determined by the ratio between the corresponding sides of the bounding boxes. Then, the scene curve is scaled to fit the model curve.\\

As the result of PCAI, we have fitted between the model and the scene. So, by using the landmarks of the model image, we can also estimate the location of the scene image. Hence, before estimating their location, the translation and rotation are applied on scene image to match the pose between the scene and the model. 

\subsection{Refinement the estimated landmarks}
The registration step provides to us the best presence between the model and the scene. The automatic landmarks can be estimated for the scene by the manual landmarks of the model. However, the estimated landmarks can be located at the incorrect location. So, we need a method to verify each position of the estimated landmarks. For this reason, in the last step of the method, we apply a step in the SIFT\cite{lowe2004distinctive} method to verify the location of automatic landmarks. Firstly, the regions around each manual and corresponding automatic landmark are created. Then, the orientation and gradient magnitude of each pixel in the region are calculated and presented into a descriptor. At the end, the comparing between the descriptors is done by using $L2$ distance which is given by the equation (1).
\begin{equation}
\label{eq:cross-correlation}
	L(x,y) = \sum\limits_{x_i,y_i}\sqrt{(x_i-y_i)^2}
\end{equation}
Where $x_i, y_i $ are the corresponding location in the descriptors.\\

The figure \ref{fig:Illustrate} is the illustration of this process. For each manual landmark on the model and corresponding estimated landmark on the scene, the small patches \textit{$I$}, \textit{$T$} are created with the size $s_1$, $s_2$, respectively ($s_1 < s_2$). Next, for each pixel in the patch \textit{$T$}, a sub-patch \textit{$T^{'}$} is extracted with the same size of the patch \textit{$I$}. Then, their descriptor and measure distance are determined(between \textit{$I$} and \textit{$T^{'}$}). This process is finished when all the pixels on the patch \textit{$T$} are considered.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.48\textwidth]{./images/illustration_SIFT}
    \caption{Illustrate the steps of descriptors comparing.}
    \label{fig:Illustrate}
\end{figure}

During the descriptor creation (\textit{$I$} and \textit{$T^{'}$}), the orientation and gradient magnitude are calculated for each their pixel by the equation:
\begin{equation}
\label{eq:sift}
\resizebox{.41 \textwidth}{!} 
{$
\begin{aligned}
	m(x,y) = \sqrt{(L(x+1,y) - L(x-1,y))^2 + (L(x,y+1) - L(x,y-1))^2} \\
	\theta(x,y) = tan^{-1}((L(x,y+1) - L(x,y-1))/(L(x+1,y) - L(x-1,y)))
	\end{aligned}
$}
\end{equation}
Where:
\begin{itemize}
	\item $m(x,y)$ is the gradient magnitude of the pixel at position (x,y)
	\item $\theta(x,y)$ is the orientation of the pixel at position (x,y)
	\item $L(x,y)$ is the value at position (x,y) in the image
\end{itemize}
Then, the descriptor is created for the patch. This is a histogram with eight direction for orientation, with the length of each bin is sum of the gradient magnitude of the pixels that corresponding  with the orientation. The descriptor is formed from a vector containing the value of all the bins in the histogram. Finally, the vector feature is modified to reduce the effects of illumination change by normalization.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{./images/keypoint_descriptor}
    \caption{A landmarks descriptor is created by computing the orientation and gradient magnitude}
    \label{fig:kpdescriptor}
\end{figure}

In the example at figure \ref{fig:kpdescriptor}, a patch with the size 16x16 is created around the landmark. For each pixel in the region, the orientation and gradient magnitude are calculated (the orientation and length of the arrows). Then, the patch is divided into 4 sub-blocks 4x4. The descriptor of each sub-block is determined by using 8-orientation histogram and the length of each arrow corresponding to the sum of the gradient magnitudes near that direction within the region.\\

As we mentioned before, the last location of
estimated landmarks is the pixel in template \textit{$T$} that has the smallest measure distance value (by $L2$ distance). Finally, the automatic landmarks are inverted to match with the real location in the scene image (by inverting the transformation and rotation).\\
\begin{figure}[h]
\centering
\subfloat[Result on right mandible]{\label{figrsmd}\includegraphics[width=0.22\textwidth]{./images/md_rs}}~~
\subfloat[Result on left mandible]{\label{figrsmg}\includegraphics[width=0.22\textwidth]{./images/mg_rs}}
\caption{The automatic landmarks on mandibles}
\label{figresult}
\end{figure}~\\
Figure \ref{figresult} shows a complete result on one scene mandible with the manual landmarks (red points) and estimated landmarks (yellow points). 
\section{Experiments and result}
All the steps in our method are implemented in MAELab\footnote{MAELab
  is a free software in C++. It can be directly obtained by request
  the authors.}. Two sets of beetle have been analyzed, right and left
mandible. After verifying the quality of the image, it remains 290
usable images for right mandible and 286 images for left mandible. The
removed images include the images that do not contain the mandible and
the mandible is broken.\\

In all valid images, a set of 18 manual landmarks of right mandible
(16 landmarks for left mandible) are indicated by biologists. Along
with choosing the centroid size to measure the mandible. This size is
obtained by sum of all square distance from each landmarks to the
centroid point (see \cite{web2010}).

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.3\textwidth]{./images/mdresult}
    \caption{The percentage of correct proportions on right mandibles }
    \label{figmdresult}
\end{figure}~\\
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.3\textwidth]{./images/mgresult}
    \caption{The percentage of correct proportions on left mandibles }
    \label{figmgresult}
\end{figure}~\\
Firstly, our evaluation is done on comparing the centroid size of the estimated landmarks and manual landmarks. The experiment is done by choosing an arbitrary 
image in the dataset as the model. The automatic landmarks are estimated on remaining images with the method that we have described. Then, the centroid size of each image is calculated and evaluated. We can see in figure
\ref{figmdresult} and figure \ref{figmgresult} for all images, the
correct proportions of centroid size which based on the estimated landmarks are 93,45\% for the right
mandible and 90,21\% for the left mandible. And the results in figure \ref{figmdresult} and \ref{figmgresult} were a vindication of the propriety of the method.\\

Besides, using the centroid size to evaluate the method. We are also
interested in the position of the estimated landmarks. In this experiment
way, we calculate the distance between each manual landmark and
corresponding automatic landmark. Through, we want to examine replacing the manual landmarks by corresponding automatic landmarks.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/md_chartlms}
    \caption{The correct proportions on each landmark of right mandibles }
    \label{figmdresultlm}
\end{figure}~\\
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/mg_chartlms}
    \caption{The correct proportions on each landmark of left mandibles }
    \label{figmgresultlm}
\end{figure}~\\
Figure \ref{figmdresultlm} and \ref{figmgresultlm} show the correct proportion on each landmark of mandible. The blue column is presented for the success rate. The orange column is expressed as the incorrect rate. With 18 landmarks of right mandible, the position of the first automatic landmarks is reasonably accurate with 98.62\%, the lowest proportion is 74,48\% for fourteenth landmark. The remaining landmarks are also indicated with a high proportion (with the accuracy proportion greater than 75\%). For left mandible, the highest and lowest success rate are 93,01\% for the first landmark and 60,14\% for the sixteenth landmark. The statistic is done on each automatic landmarks of all the images.\\

In both of experiments, the success rate on right mandible is always greater than left mandible. So, when we reconsider the datasets, the images in left mandible are having more type of size than on the right mandible (scale problem). This explains why the success rate on right mandible is always higher than left mandible.\\

From two experiment ways, we can see that the method is success in
indicating all landmarks for each image; and the location of the
landmarks is considered near with the manual landmarks in some
aspect. In a different side, when comparing with our previous study (see in
\cite{leestimating}). This method has more exactly about the position of
automatic landmarks as well as the advantages for the implementation
process. The memory to detecting the landmarks, along with the times
to execute the process are decreased dramatically.

\section{Conclusion}
Morphometric analysis is a powerful tool in biology in classification
the species. Automatic identification the characteristics biology of
the organism is a difficult problem. In the content of this paper, we
have begun to design a method to segment the beetle mandibles and to
indicate automatically landmarks which have been determined by
biologists. Each mandible is segmented by applying the Canny
algorithm. Using PCAI to align the images and estimate the
landmarks. Finally, a descriptor distance will be applied to refine the
location of the estimated landmarks. The first version of this method
has been implemented. From now, the next stage of our method is to add
the features to have the position of landmarks more precisely, i.e diagnose on the scale of the image.


\bibliographystyle{plain}
\bibliography{references}

\end{document}

%-------------------------------------------------------------------------
% example of algorithm typesetting
% to allow this, uncomment line 
% \RequirePackage[noend]{myalgorithm}
% in the wscg.sty file
% and download that package from Gabriel Zachmann's page http://zach.in.tu-clausthal.de/latex/
%
%
%\begin{algorithm}
%\hrule
%  \centering
%\begin{algorithmic}
%    \STMT $d_{l,r} = f_B(P_1), f_B(P_n)$
%    \WHILE{ $|d_l| > \epsilon $ and $|d_r| > \epsilon $ and $l<r$}
%        \STMT $d_x = f_B(P_x)$
%        \IF{ $d_x < 0$ }
%            \STMT $l, r = x, r$
%        \ELSE
%            \STMT $l, r = l, x$
%        \ENDIF
%    \ENDWHILE
%\end{algorithmic}
%\hrule
%\caption{Example of some pseudo-code}
%\label{fg:code}
%\end{algorithm}

%-------------------------------------------------------------------------

\begin{thebibliography}{99}
\label{references}
\bibitem[1]{canny} Canny, John. "A computational approach to edge detection." IEEE Transactions on pattern analysis and machine intelligence 6 (1986): 679-698.
\bibitem[2]{Ballard} Ballard, Dana H. "Generalizing the Hough transform to detect arbitrary shapes." Pattern recognition 13.2 (1981): 111-122.
\bibitem[3]{Webster} Webster, M. A. R. K., and H. DAVID Sheets. "A practical introduction to landmark-based geometric morphometrics." Quantitative Methods in Paleobiology 16 (2010): 168-188.
\bibitem[4]{est} Le Van, L., et al. "Estimating landmarks on 2D images of beetle mandibles."
\bibitem[5]{pca} Shlens, Jonathon. "A tutorial on principal component analysis." arXiv preprint arXiv:1404.1100 (2014).
\bibitem[6]{sift} Lowe, David G. "Distinctive image features from scale-invariant keypoints." International journal of computer vision 60.2 (2004): 91-110.
\bibitem[7]{palaniswamy} Palaniswamy, Sasirekha, Neil A. Thacker, and Christian Peter Klingenberg. "Automatic identification of landmarks in digital images." IET Computer Vision 4.4 (2010): 247-260.
\end{thebibliography}

%{\bfseries
%Last page should be fully used by text, figures etc. Do not leave empty space, please. 

%Do not lock the PDF -- additional text and info will be inserted, i.e. ISSN/ISBN etc. 
%}

